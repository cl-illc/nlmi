{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**\n",
    "\n",
    "* [Random sentences](#snt)\n",
    "* [Language models](#lm)\n",
    "* [Implementation](#imp)\n",
    "    * [Unigram LM](#imp-uni)\n",
    "    * [A general n-gram language model](#imp-n)\n",
    "* [Evaluation](#eval)\n",
    "* [Smoothing](#smooth)\n",
    "* [Interpolation](#inter)\n",
    "    \n",
    "**Table of Exercises**\n",
    "\n",
    "* [Exercise 3-1](#ex3-1) (-/2)\n",
    "* [Exercise 3-2](#ex3-2) (-/3)\n",
    "* [Exercise 3-3](#ex3-3) (-/2)\n",
    "* [Exercise 3-4](#ex3-4) (-/1)\n",
    "* [Exercise 3-5](#ex3-5) (-/10)\n",
    "* [Exercise 3-6](#ex3-6) (-/2)\n",
    "* [Exercise 3-7](#ex3-7) (-/3)\n",
    "* [Exercise 3-8](#ex3-8) (-/2)\n",
    "* [Exercise 3-9](#ex3-9) (/-3)\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* develop ngram language models\n",
    "* estimate parameters of LMs via MLE\n",
    "* evaluate LMs intrinsically in terms of perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"snt\"> Random sentences\n",
    "\n",
    "Given a sentence $S$ (for example  ***He went to the store*** in English) a language model (LM) can tell us if this resembles a natural sentence.\n",
    "Can we learn a model to asses the fluency of sentences generated by an automatic system?\n",
    "\n",
    "For example, such a model must prefer a sentence like ***He went to the store*** to a sentence like ***He store go***.\n",
    "\n",
    "A language model is an attempt at quantifying a notion of the degree of goodness (or badness) of\n",
    "any given sentence. The best way to represent this degree of goodness is as a probability value $P$, where, if the model assigns a high probability value to *He went to the store*, it can be concluded that this sentence is much more likely to be a fluent English sentence than *He store go* which is assigned a low probability.\n",
    "\n",
    "We model a sentence $S$ as a sequence of random words, so let's first define a random variable $X$ that represents a random word:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3-1\" style=\"color:red\">**Exercise 3-1**</a> **[2 points]** Define a categorical random variable $X$ for words sampled from a closed vocabulary $\\Sigma$ (assume the size of the vocabulary is denoted by $v$). In your answer make sure you indicate what is the sample space and the precise support $\\mathcal X$ of the categorical random variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **sentence** corresponds to any sequence of words in $\\Sigma^*$.\n",
    "\n",
    "We denote a random sentence $S$ by the sequence $\\langle X_1, \\ldots, X_n \\rangle$ or the shorthand $X_1^n$.\n",
    "The following definition is also useful:\n",
    "\n",
    "* for the $i$th random word $X_i$, the prefix $\\langle x_1, \\ldots, x_{i-1} \\rangle$ (also denoted $x_{<i}$) is called a random history.\n",
    "* we use $H$ to denote an arbitrary random history and $H_i$ to denote the $i$th random history\n",
    "\n",
    "A **generative story** is a stochastic procedure that we define as a means to explain the process by which we believe data are generated. For random sentences we define the following generative story\n",
    "\n",
    "1. Sample the sequence length from a distribution $P_N$\n",
    "    * $N \\sim P_N$\n",
    "2. Then for each position $i=1 , \\ldots, n$ sample the $i$th word from the distribution  $P_{X|H}$\n",
    "    * $X_i|x_{<i} \\sim P_{X|H=x_{<i}}$\n",
    "\n",
    "Here is an example for a sentence with $3$ words:\n",
    "\n",
    "$P_S(\\langle x_1, x_2, x_3 \\rangle) = P_N(3) P_{X|H}(x_1) P_{X|H}(x_2|\\langle x_1 \\rangle) P_{X|H}(x_3 | \\langle x_1, x_2 \\rangle)$\n",
    "\n",
    "For our example sentence *He went to the store* this means:\n",
    "\n",
    "$P(\\text{\"He went to the store\"}) = P_N(5) \\times P(\\text{He}) \\times P(\\text{went}|\\langle \\text{He} \\rangle) \\times P(\\text{to}|\\langle \\text{He}, \\text{went} \\rangle) \\times P(\\text{the}|\\langle \\text{He},  \\text{went}, \\text{to} \\rangle) \\times P(\\text{store}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the} \\rangle) $\n",
    "\n",
    "* where with some abuse of notation we use the words themselves instead of their corresponding indices. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3-2\" style=\"color:red\">**Exercise 3-2**</a> **[3 points]**  Write down the general rule for the probability $P_S$ of a sentence $x_1^n$. For this exercise please use subscripts to indicate the precise random variable associated with every distribution (that is, for example, $P_S$ is correct while $P$ is wrong). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"lm\"> Language models\n",
    "\n",
    "Here we quickly revisit the material discussed in class about n-gram LMs.\n",
    "\n",
    "We start with the simplest unigram language model. The idea is to forget the history therefore making a strong independence assumption:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "(1) \\qquad P_S(x_1^n) \\approx P_N(n) \\prod_{i=1}^n P_X(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "* we assume $P_N(n)$ to be some constant $c$, this means that we have a uniform distribution over length\n",
    "* and we assume $P_X$ to be a Categorical distribution\n",
    "\n",
    "Thus, the final <a name=\"eq-unigram-lm\">unigram LM definition</a> is \n",
    "\n",
    "\\begin{equation}\n",
    "(2) \\qquad P_S(x_1^n; c, \\theta_1^v) \\triangleq c \\prod_{i=1}^n \\text{Cat}(X=x_i|\\theta_1, \\ldots, \\theta_v)\n",
    "\\end{equation}\n",
    "\n",
    "Note that we have introduced the Categorical pmf, which you have learnt about in Lab2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3-3\" style=\"color:red\">**Exercise 3-3**</a> **[2 points]**  Complete the categorical probability mass function (pmf) and the conditions below:\n",
    "\n",
    "$\\text{Cat}(X=a|\\theta_1, \\ldots, \\theta_v) = $ *type the pfm*\n",
    "\n",
    "where $\\theta_1^v$ are the categorical parameters for which it must hold\n",
    "\n",
    "1. *type condition 1*\n",
    "2. *type condition 2*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum likelihood estimation**\n",
    "\n",
    "Suppose we are given a corpus containing $m$ sentences\n",
    "\n",
    "* $\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle$ for $k=1, \\ldots, m$\n",
    "* where $n_k$ is the length of the $k$th sentence\n",
    "\n",
    "The MLE solution for the unigram LM is based on gathering counts and computing the relative frequency of word types:\n",
    "\n",
    "\\begin{equation}\n",
    "(3) \\qquad \\theta_x = \\frac{\\text{count}(x)}{\\text{number of tokens}}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the *number of tokens* is simply the sum of the length of the sentences $\\sum_{k=1}^m n_k$.\n",
    "\n",
    "\n",
    "More generaly for a conditional probability distribution (cpd), we have that \n",
    "\n",
    "* $P_{X|H}(x|h) = \\text{Cat}(X=x|\\theta_1^{(h)}, \\ldots, \\theta_v^{(h)})$\n",
    "\n",
    "where $h$ uniquely indexes a history and $P_{X|H}(x|h) = \\theta_x^{(h)}$ is the $x$th probability value in the $h$th cpd.\n",
    "\n",
    "Then the MLE solution is simply\n",
    "\n",
    "\\begin{equation}\n",
    "(4) \\qquad \\theta_x^{(h)} = \\frac{\\text{count}(h \\circ \\langle x \\rangle)}{\\text{count}(h)}\n",
    "\\end{equation}\n",
    "\n",
    "where  $h \\circ \\langle x \\rangle$ is the concatenation of history and word.\n",
    "\n",
    "Now that we know how to estimate general cpds we can define the n-gram LM.\n",
    "\n",
    "An <a name=\"eq-ngram-lm\">$n$-gram LM</a> is a Markov model of order $o=n-1$ where we truncate the complete history $x_{<i}$ so that it contains only the $o$ most recent words $x_{i-o}^{i-1}$.\n",
    "\n",
    "\\begin{equation}\n",
    "(5) \\qquad P_S(x_1^n; c, \\boldsymbol \\theta) \\triangleq c \\prod_{i=1}^n P_{X|H}(x_i|x_{i-o}^{i-1}; \\boldsymbol \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $P_{X|H=h; \\boldsymbol \\theta}$ is $\\text{Cat}(\\theta_1^{(h)}, \\ldots, \\theta_v^{(h)})$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Example***\n",
    "\n",
    "Consider the sentence *He went to the store*, its probability under the unigram LM is\n",
    "\n",
    "$P_S(\\langle \\text{He, went, to, the, store} \\rangle) \\propto P_X(\\text{He}) \\times P_X(\\text{went}) \\times P_X(\\text{to}) \\times P_X(\\text{the}) \\times P_X(\\text{store})$\n",
    "\n",
    "which can also be seen as \n",
    "\n",
    "$P_S(\\langle \\text{He, went, to, the, store} \\rangle) \\propto \\theta_{\\text{He}} \\times \\theta_{\\text{went}} \\times \\theta_{\\text{to}} \\times \\theta_{\\text{the}} \\times \\theta_{\\text{store}}$\n",
    "\n",
    "where again we use the words instead of their indices and we use the proportionality symbol to ignore the probability of the length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3-4\" style=\"color:red\">**Exercise 3-4**</a> **[1 point]**  Write down the probability of the sentence \n",
    "\n",
    "    He went to the store\n",
    "    \n",
    "under a bigram language model. Tip: recall that *the* is a word while $\\langle \\text{the} \\rangle$ is a sequence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a name=\"imp\"> Implementation\n",
    "\n",
    "We will start by showing you how to implement the unigram LM. \n",
    "\n",
    "Consider the PTB dataset as our training data: *sec02-21.raw*. We will estimate the categorical parameters and we query the LM with some sentences to find out their probability.\n",
    "\n",
    "Notes: \n",
    "\n",
    "1. For *memory efficiency* rather than vectors we will use sparse data structures (such as python dict), this is nice because we do not use memory to represent events that have never occurred.\n",
    "2. We lowercase the data to collect better statistics (otherwise 'He' and 'he' would correspond to different words)\n",
    "3. Recall from the lecture that we pad sentences with 1 EOS token (which becomes part of the sequence) and $n-1$ BOS tokens (which are there just to make the history size constant). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"imp-uni\"> Unigram LM\n",
    "\n",
    "We start with the unigram language model, whose factorisation is shown in [Equation (2)](#eq-unigram-lm).\n",
    "\n",
    "First, we start by **loading and pre-processing data**. In the code below we use [python generators](https://wiki.python.org/moin/Generators), check the link if you are not familiar with then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict  # this is a very useful modification of a dict, check the documentation\n",
    "\n",
    "\n",
    "def preprocess(file_path, char_level=False):\n",
    "    \"\"\"\n",
    "    Returns a generator (a data stream) that yields one pre-processed sentence at a time.\n",
    "    A preprocessed sentence is:\n",
    "        - a list of tokens (each token a string)\n",
    "            - where tokens are lowercased\n",
    "        \n",
    "    :param file_path: path to a text corpus\n",
    "    :returns: a generator of sentences\n",
    "        A generator is an object that can be used in `for` loops\n",
    "    \"\"\"\n",
    "    count = defaultdict(int)\n",
    "    # First we count the number of occurrences of each token\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # we skip empty lines\n",
    "            if char_level:\n",
    "                sentence = [ch for ch in line.lower()] \n",
    "            else:\n",
    "                sentence = line.lower().split()\n",
    "            for token in sentence:\n",
    "                count[token] += 1\n",
    "    # then we yield one preprocessed sentence at a time\n",
    "    # making sure we map infrequent tokens to <unk>\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # we skip empty lines\n",
    "            if char_level:\n",
    "                sentence = [ch for ch in line.lower()] \n",
    "            else:\n",
    "                sentence = line.lower().split()\n",
    "            yield sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our preprocessed sentence generator\n",
    "for k, sentence in enumerate(preprocess('eleanor-rigby.txt'), 1):\n",
    "    print(k, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show you **how to count unigrams**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def count_unigrams(sentence_stream):\n",
    "    \"\"\"\n",
    "    input: a generator of preprocessed sentences\n",
    "        - a preprocessed sentence is a list of lowercased tokens\n",
    "    output: \n",
    "        unigram_count: dictionary of frequency of each word\n",
    "    \"\"\"    \n",
    "    unigram_counts = defaultdict(int)\n",
    "    for sentence in sentence_stream:\n",
    "        sentence = sentence + [\"</s>\"]  # add end of sentence\n",
    "        for token in sentence:\n",
    "            unigram_counts[token.lower()] += 1  # frequency of each word\n",
    "    return unigram_counts\n",
    "\n",
    "# Let's test our procedure and check how many times 'cat' and 'mat' happen in the PTB training corpus\n",
    "unigram_count_table =  count_unigrams(preprocess('sec02-21.raw'))  \n",
    "print('unigram=cat count=%d' % unigram_count_table['cat'])\n",
    "print('unigram=mat count=%d' % unigram_count_table['mat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show you **how to get the MLE solution for the unigram distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_mle(unigram_counts):\n",
    "    \"\"\"\n",
    "    input: unigram_count: dictionary of frequency of each word\n",
    "           \n",
    "    output: unigram_prob: dictionary with the probabilty of each word \n",
    "            (parameters of the model)\n",
    "    \"\"\"\n",
    "    total_count = sum(unigram_count_table.values())\n",
    "    unigram_probs = defaultdict(float)\n",
    "    for word, count in unigram_counts.items():\n",
    "        unigram_probs[word] = float(count) / total_count\n",
    "    return unigram_probs\n",
    "\n",
    "# Let's check the MLE parameters associated with 'cat' and 'mat' by querying their unigram probabilities\n",
    "unigram_prob_table = unigram_mle(unigram_count_table)\n",
    "print('unigram=cat prob=%f' % unigram_prob_table['cat'])\n",
    "print('unigram=mat prob=%f' % unigram_prob_table['mat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we show you **how to compute the log-probability** of a sentence under the unigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_sentence_unigram_log_probability(sentence, word_probs):\n",
    "    \"\"\"\n",
    "    input: list of words in a sentence\n",
    "    word_probs: MLE paremeters\n",
    "    output:\n",
    "            sentence_probability_sum: log probability of the sentence\n",
    "    \"\"\"\n",
    "    sentence_probability_sum = 0.\n",
    "    # we first get the probability of unknown words\n",
    "    #  which by default is 0. in case '<unk>' is not in the support\n",
    "    unk_probability = word_probs.get('<unk>', 0.)\n",
    "    for word in sentence:\n",
    "        # this will return `unk_probability` if the word is not in the support\n",
    "        word_probability = word_probs.get(word, unk_probability)  \n",
    "        # it is a sum of log pboabilities\n",
    "        # we use np.log because it knows that log(0) is float('-inf')\n",
    "        sentence_probability_sum += np.log(word_probability)\n",
    "    return sentence_probability_sum  \n",
    "    \n",
    "sent_prob = calculate_sentence_unigram_log_probability(['the', 'cat', 'sat', 'on', 'the', 'cat'], unigram_prob_table)\n",
    "print(sent_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unseen words**\n",
    "\n",
    "However, note that if we want the probability of sentences containing words that are not present in the training corpus, we will have an unpleasant surprise. \n",
    "\n",
    "For example: *the cat sat on the mat*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sentence_unigram_log_probability(['the', 'cat', 'sat', 'on', 'the', 'mat'], unigram_prob_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course that would not happen if we had smoothed our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"imp-n\"> A general n-gram language model\n",
    "\n",
    "We now turn to a general $n$-gram LM, whose factorisation is shown in [Equation (5)](#eq-ngram-lm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3-5\" style=\"color:red\">**Exercise 3-5**</a> **[10 points]** In this exercise you will build a general $n$-gram language model where $n \\ge 1$. We provide you with a skeleton class on which to build. \n",
    "\n",
    "a) Implementation **[3 points]**\n",
    "\n",
    "* Start by implementing the method `count_ngrams`, see the documentation of the method for specification. Tip: expand upon the procedure implemented in the function `count_unigrams` above; remember to handle BOS tokens and EOS tokens correctly. Use `<s>` for BOS token and `</s>` for EOS token.\n",
    "* Now implement the method `solve_mle`, see the documentation of the method for specification.\n",
    "* Finally, implement the `log_prob` method, see the documentation of the method for specification.\n",
    "\n",
    "b) Toy data **[1 point]**: Train 3 models (unigram, bigram, trigram) using Eleanor Rigby's lyrics (`eleanor-rigby.txt`) and show that you can reproduce the output of the code below.\n",
    "\n",
    "```python\n",
    "unigram_lm = LM(order=0)\n",
    "bigram_lm = LM(order=1)\n",
    "trigram_lm = LM(order=2)\n",
    "\n",
    "unigram_lm.count_ngrams(preprocess('eleanor-rigby.txt'))\n",
    "unigram_lm.solve_mle()\n",
    "bigram_lm.count_ngrams(preprocess('eleanor-rigby.txt'))\n",
    "bigram_lm.solve_mle()\n",
    "trigram_lm.count_ngrams(preprocess('eleanor-rigby.txt'))\n",
    "trigram_lm.solve_mle()\n",
    "print(unigram_lm.log_prob(\"where do they all belong ?\".split()))\n",
    "print(bigram_lm.log_prob(\"where do they all belong ?\".split()))\n",
    "print(trigram_lm.log_prob(\"where do they all belong ?\".split()))\n",
    "```\n",
    "\n",
    "which should produce\n",
    "\n",
    "```python\n",
    "-23.5871446234\n",
    "-3.56272816879\n",
    "-2.42774823595\n",
    "```\n",
    "\n",
    "c) PTB data (`sec02-21.raw`): train 3 models (unigra, bigram, and trigram) and report the probability of: <span style=\"color:blue\">the new rate will be payable feb. 15 .</span>\n",
    "\n",
    "* unigram model **[2 points]**\n",
    "* bigram model **[2 points]**\n",
    "* trigram model **[2 points]**\n",
    "\n",
    "The excerpt below\n",
    "\n",
    "```python\n",
    "unigram_lm = LM(order=0)\n",
    "bigram_lm = LM(order=1)\n",
    "trigram_lm = LM(order=2)\n",
    "\n",
    "unigram_lm.count_ngrams(preprocess('sec02-21.raw'))\n",
    "unigram_lm.solve_mle()\n",
    "bigram_lm.count_ngrams(preprocess('sec02-21.raw'))\n",
    "bigram_lm.solve_mle()\n",
    "trigram_lm.count_ngrams(preprocess('sec02-21.raw'))\n",
    "trigram_lm.solve_mle()\n",
    "print(unigram_lm.log_prob(\"the new rate will be payable feb. 15 .\".split()))\n",
    "print(bigram_lm.log_prob(\"the new rate will be payable feb. 15 .\".split()))\n",
    "print(trigram_lm.log_prob(\"the new rate will be payable feb. 15 .\".split()))\n",
    "```\n",
    "\n",
    "should produce\n",
    "\n",
    "```python\n",
    "-63.0944350135\n",
    "-35.0096672791\n",
    "-20.6963911844\n",
    "```\n",
    "\n",
    "*Help with debugging?*\n",
    "\n",
    "We have provided a toy corpus called `eleanor-rigby.txt`, for that corpus we provided the output of `print_count_table` and `print_prob_table` for a correct implementation of the LM class. We have varied order from 0 to 2:\n",
    "\n",
    "* `eleanor-rigby-unigram-counts.txt`\n",
    "* `eleanor-rigby-unigram-cpd.txt`\n",
    "* `eleanor-rigby-bigram-cpds.txt`\n",
    "* `eleanor-rigby-bigram-counts.txt`\n",
    "* `eleanor-rigby-trigram-cpds.txt`\n",
    "* `eleanor-rigby-trigram-counts.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "class LM:\n",
    "    \n",
    "    def __init__(self, order):\n",
    "        self._order = order\n",
    "        self._count_table = dict()\n",
    "        self._prob_table = dict()\n",
    "        self._vocab = set()\n",
    "        \n",
    "    def order(self):\n",
    "        return self._order\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self._vocab)\n",
    "        \n",
    "    def print_count_table(self, output_stream=sys.stdout):\n",
    "        \"\"\"Prints the count table for visualisation\"\"\"\n",
    "        for history, ngrams in sorted(self._count_table.items(), key=lambda pair: pair[0]):\n",
    "            for word, count in sorted(ngrams.items(), key=lambda pair: pair[0]):\n",
    "                print('history=\"%s\" word=%s count=%d' % (' '.join(history), word, count), file=output_stream)\n",
    "                \n",
    "    def print_prob_table(self, output_stream=sys.stdout):\n",
    "        \"\"\"Prints the tabular cpd for visualisation\"\"\"\n",
    "        for history, ngrams in sorted(self._prob_table.items(), key=lambda pair: pair[0]):\n",
    "            for word, prob in sorted(ngrams.items(), key=lambda pair: pair[0]):\n",
    "                print('history=\"%s\" word=%s prob=%f' % (' '.join(history), word, prob), file=output_stream)\n",
    "                \n",
    "    def preprocess_history(self, history):\n",
    "        \"\"\"\n",
    "        This function pre-process an arbitrary history to match the order of this language model.\n",
    "        :param history: a sequence of words\n",
    "        :return: a tuple containing exactly as many elements as the order of the model\n",
    "            - if the input history is too short we pad it with <s> \n",
    "        \"\"\"\n",
    "        if len(history) == self._order:\n",
    "            return tuple(history)\n",
    "        elif len(history) > self._order:\n",
    "            length = len(history)            \n",
    "            return tuple(history[length - self._order: length])\n",
    "        else:  # here the history is too short\n",
    "            missing = self._order - len(history)\n",
    "            return tuple(['<s>'] * missing) + tuple(history)\n",
    "                \n",
    "    def get_parameter(self, history, word):\n",
    "        \"\"\"\n",
    "        This function returns the categorical parameter associated with a certain word given a certain history.\n",
    "        :param history: a sequence of words (a tuple)\n",
    "        :param word: a word (a str)\n",
    "        :return: a float representing P(word|history)\n",
    "        \"\"\"\n",
    "        history = self.preprocess_history(history)\n",
    "        cpd = self._prob_table.get(history, None)\n",
    "        if cpd is None:\n",
    "            return 0.\n",
    "        else:\n",
    "            # we either return P(x|h)\n",
    "            #  or P(unk|h) in case x is not in the support of this cpd\n",
    "            #   or 0. in case neither x nor unk are in the support of this cpd\n",
    "            unk_probability = cpd.get('<unk>', 0.)\n",
    "            return cpd.get(word, unk_probability)\n",
    "        \n",
    "    def cpd_items(self, history):\n",
    "        history = self.preprocess_history(history)\n",
    "        # if the history is unseen we return an empty cpd\n",
    "        return self._prob_table.get(history, dict()).items()\n",
    "        \n",
    "    def count_ngrams(self, data_stream):\n",
    "        \"\"\"\n",
    "        This function should populate the attribute _count_table which should be understood as \n",
    "            - a python dict \n",
    "                - whose key is a history (a tuple of words)\n",
    "                - and whose value is itself a python dict (or defaultdict)\n",
    "                    - which maps a word (a string) to a count (an integer)\n",
    "        \n",
    "        This function will add counts to whatever counts are already stored in _count_table.\n",
    "        \n",
    "        This function also maintains a unique set of words in the vocabulary using the attribute _vocab\n",
    "        \n",
    "        :param data_stream: a generator as produced by `preprocess`\n",
    "        \"\"\"\n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass        \n",
    "                    \n",
    "    def solve_mle(self):\n",
    "        \"\"\"\n",
    "        This function should compute the attribute _prob_table which has the exact same structure as _count_table\n",
    "         but stores probability values instead of counts. \n",
    "        It can be seen as the collection of cpds of our model, that is, _prob_table\n",
    "            - maps a history (a tuple of words) to a dict where\n",
    "                - a key is a word (that extends the history forming an ngram)\n",
    "                - and the value is the probability P(word|history)                \n",
    "                \n",
    "        This function will replace whatever value _prob_table currently stores by the newly computed MLE solution.\n",
    "        \"\"\"\n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass        \n",
    "        \n",
    "    def log_prob(self, sentence):\n",
    "        \"\"\"\n",
    "        Compute the log probability of a sentence under this model. \n",
    "                \n",
    "        input: \n",
    "            sentence: a sequence of tokens\n",
    "        output:\n",
    "            log probability\n",
    "        \"\"\"\n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"eval\">  Evaluation\n",
    "\n",
    "The way to evaluate the performance of a LM is to test into a final application. In other words, how much the final score of the application improves. This is called *extrinsic* evaluation. Also, we can test our LM independently from an application, this is called *intrinsinc* evaluation. In this course, we are going to study the intrinsic evaluation of the LM.\n",
    "\n",
    "To test a LM model we prepare 3 datasets: \n",
    "    Training is used for estimating $\\boldsymbol \\theta$ (we use boldface to indicate a collection of parameters).\n",
    "    Develpment is used to make choices across models.\n",
    "    Test is used for measuring the accuracy of the model.\n",
    "   \n",
    "In n-gram LM the evaluation is defined by the **likelihood** of the model with respect of the test dataset.\n",
    "The likelihood of the parameters $\\theta$ over the test dataset is the probability that the model assigns to the dataset.\n",
    "\n",
    "We assume the test data $\\mathcal T$ consits of $m$ independent sentences each denoted $\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle$ \n",
    "\n",
    "$P(\\mathcal T; \\boldsymbol \\theta) = \\prod_{k=1}^m P_S(\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle; \\boldsymbol \\theta)$\n",
    "\n",
    "Or in form of the log-likelihood:\n",
    "\n",
    "$\\log P(\\mathcal T; \\theta) = \\sum_{k=1}^m \\log P_N(n_k) + \\log P_{S|N}(\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle|n_k; \\theta)$\n",
    "\n",
    "We assume the length probability to be constant, so in comparing different models that probability does not make a difference. Thus we drop it and define the log-likelihood as follows:\n",
    "\n",
    "$\\mathcal L(\\boldsymbol \\theta) = \\sum_{k=1}^m \\log P_{S|N}(\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle|n_k; \\boldsymbol \\theta)$\n",
    "\n",
    "\n",
    "Then the model that assings the higher $\\mathcal L$ to the test set is the one that best fits the data. In other words,  given two probabilistic models, the better model is the one that assigns a higher probability to the test data. One detail we need to abstract away from is differences in factorisation of the models which may cause their likelihoods not to be comparable, but for that we will define *perplexity* below. \n",
    "\n",
    "The log likelihood is used because the probability of a particular sentence according to the LM can be a very small number, and the product of these small numbers can become even smaller, and it will cause numerical\n",
    "precision problems. \n",
    "\n",
    "\n",
    "**Perplexity** of a language model on a test set is the inverse probability of the test set, normalized\n",
    "by the number of tokens. Perplexity is a notion of average branching factor, thus a LM with low perplexity can be thought of as a *less confused* LM. That is, each time it introduces a word given some history it picks from a reduced subset of the entire vocabulary (in other words, it is more certain of how to continue). \n",
    "\n",
    "If a dataset contains $t$ tokens where $t = \\sum_{k=1}^m n_k$, then the perplexity of the dataset is\n",
    "\n",
    "\\begin{equation}\n",
    "(6) \\qquad \\text{PP}(\\mathcal T) = \\left( \\prod_{k=1}^m P_{S|N}(\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle|n_k; \\boldsymbol \\theta) \\right)^{-1/t}\n",
    "\\end{equation}\n",
    "\n",
    "where we have already discarded the length distribution (since it's held constant across models).\n",
    "\n",
    "It's again convenient to use log and define log-perplexity\n",
    "\n",
    "\\begin{equation}\n",
    "(7) \\qquad \\log \\text{PP}(\\mathcal T) = - \\frac{1}{t} \\sum_{k=1}^m \\log P_{S|N}(\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle|n_k; \\boldsymbol \\theta) \n",
    "\\end{equation}\n",
    "\n",
    "You can compare models in terms of the log-perplexity they assign to the same test data. The lower the perplexity, the better the model is.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's quickly make a helper function to load test data\n",
    "#  and segment lines into sequences of lowercased tokens\n",
    "def make_test_generator(path, char_level=False):\n",
    "    \"\"\"Return a generator for test sentences\"\"\"\n",
    "    with open(path, 'r') as fi:\n",
    "        for line in fi:\n",
    "            if char_level:\n",
    "                yield [ch for ch in line.lower()]\n",
    "            else:\n",
    "                yield line.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"ex3-6\" style=\"color:red\">**Exercise 3-6**</a> Implement the log-perplexity function below. See the function documentation for specifications.\n",
    "\n",
    "* Two sentences test **[2 points]**: If you run the excerpt below for models trained on PTB\n",
    "\n",
    "```python\n",
    "two_sentences_data = [\n",
    "    \"Ms. Haag plays Elianti .\".lower().split(),\n",
    "    \"Apparently the commission did not really believe in this ideal .\".lower().split()\n",
    "]\n",
    "\n",
    "log_ppl = log_perplexity(two_sentences_data, unigram_lm)\n",
    "print(log_ppl)\n",
    "log_ppl = log_perplexity(two_sentences_data, bigram_lm)\n",
    "print(log_ppl)\n",
    "log_ppl = log_perplexity(two_sentences_data, trigram_lm)\n",
    "print(log_ppl)\n",
    "```\n",
    "\n",
    "and your implementation is correct, you will get\n",
    "```\n",
    "7.32267906044\n",
    "3.87958613355\n",
    "2.15917055083\n",
    "```\n",
    "\n",
    "At this point if your try to evaluate the perplexity of the PTB test set `sec00.raw` \n",
    "```python\n",
    "\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), unigram_lm))\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), bigram_lm))\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), trigram_lm))\n",
    "```\n",
    "\n",
    "you will get `inf` for all models. That's because you need to implement smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_perplexity(data_stream, lm):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of the given text.\n",
    "    This is simply 2 ** cross-entropy for the text.\n",
    "    \n",
    "    This function can make use of `lm.order()`, `lm.get_parameter()`, and `lm.log_prob()` \n",
    "\n",
    "    :param data_stream: generator of sentences (each sentence is a list of words)\n",
    "    :param lm: an instance of the class LM\n",
    "    \"\"\"\n",
    "    # TYPE YOUR SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"smooth\"> Smoothing\n",
    "\n",
    "Note that MLE will fail if we evaluate on sentences containing n-grams that the model has never seen (at training). For example, *He went to the store* some bigrams are not present in the corpus giving a probability of *zero*.\n",
    "\n",
    "The words we haven't seen before are called unknown words, or out of vocabulary (OOV) words.\n",
    "We will now map them to a special symbol such as `<unk>`.\n",
    "    \n",
    "To keep the LM from assigning zero probability to these unseen events (ngrams), we’ll have to steal some of the probability mass from some more frequent events and give it to the events we've never seen.\n",
    "This is called **smoothing** or **discounting**.\n",
    "\n",
    "The simplest form of smoothing is called **Laplace smoothing**, whereby we add `<unk>` to the support of the distribution and then add one to all counts before we normalize them into probabilities. \n",
    "All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. \n",
    "\n",
    "We can also generalise it and add $\\alpha$ instead of $1$. Then for $P_{X|H=h} = \\text{Cat}(\\theta_1^{(h)}, \\ldots, \\theta_v^{(h)})$ we get the MLE solution:\n",
    "\n",
    "\\begin{equation}\n",
    "(7) \\qquad \\theta_x^{(h)} = \\frac{ \\text{count}(h \\circ \\langle x \\rangle) + \\alpha}{\\text{count}(h) + v \\alpha}\n",
    "\\end{equation}\n",
    "\n",
    "There are $v$ words in the vocabulary and each one was incremented by $\\alpha$, we also need to adjust the denominator to take into account the extra $v\\alpha$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3-7\" style=\"color:red\">**Exercise 3-7**</a> **[3 points]**\n",
    "\n",
    "Complete the `LaplaceLM` class below. Note that it must extend from `LM`. Implement the 3 modifications below in order to obtain add $\\alpha$ smoothing. \n",
    "\n",
    "1. **[1 point]** Modify `count_ngrams` to add `<unk>` to the support of every cpd (that is, for every possible history, including the empty history, an `<unk>` outcome with count 0 should exist.\n",
    "2. **[1 point]** Modify `solve_mle` so that it adds $\\alpha$ to every count before normalisation.\n",
    "3. **[1 point]** Modify `get_parameter` so that it returns $\\alpha/v$ when the history is unknown, that is, when $\\text{count}(h)$ is $0$\n",
    "\n",
    "To get all points your you need to show that your code can reproduce the following result.\n",
    "\n",
    "If your implementation is correct, for add $1$ smoothing, the following excerpt of code\n",
    "```python\n",
    "unigram_lm_laplace = LaplaceLM(order=0, alpha=[1.])\n",
    "bigram_lm_laplace = LaplaceLM(order=1, alpha=[1., 1.])\n",
    "\n",
    "unigram_lm_laplace.count_ngrams(preprocess('sec02-21.raw'))\n",
    "unigram_lm_laplace.solve_mle()\n",
    "bigram_lm_laplace.count_ngrams(preprocess('sec02-21.raw'))\n",
    "bigram_lm_laplace.solve_mle()\n",
    "\n",
    "\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), unigram_lm_laplace))\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), bigram_lm_laplace))\n",
    "```\n",
    "\n",
    "should produce\n",
    "\n",
    "```python\n",
    "7.06497838227128\n",
    "8.122959052804662\n",
    "```\n",
    "\n",
    "As you can see, Laplace smoothing improved the language models by assigning a non-zero probability to sentences with unseen words and/or bigrams. However, you can see that smoothing is not as effective as ngrams get longer. In fact the assumption behind smoothing becomes less and less realistic as the history grows longer.\n",
    "\n",
    "For example, if our implementation is modified to support a different smoothing coefficient per length of history, we can smooth bigram counts less than we smooth unigram counts and this in turn improves the bigram LM.\n",
    "\n",
    "```python\n",
    "bigram_lm_laplace2 = LaplaceLM(order=1, alpha=[1., 0.01])  # note the change of smoothing coefficient for bigrams\n",
    "bigram_lm_laplace2.count_ngrams(preprocess('sec02-21.raw'))\n",
    "bigram_lm_laplace2.solve_mle()\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), bigram_lm_laplace2))\n",
    "```\n",
    "\n",
    "should produce\n",
    "\n",
    "```python\n",
    "6.6531604494059815\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LaplaceLM(LM):\n",
    "    \n",
    "    def __init__(self, order, alpha=1.):\n",
    "        \"\"\"\n",
    "        order: the Markov assumption\n",
    "        alpha: list of coefficients (one per length of history)\n",
    "        \"\"\"\n",
    "        super(LaplaceLM, self).__init__(order)\n",
    "        assert len(alpha) == order + 1, 'we need smoothing coefficients for all history lengths'\n",
    "        self._alpha = alpha   \n",
    "        # in Laplace smoothing we always add '<unk>' to the vocabulary\n",
    "        self._vocab.add('<unk>')\n",
    "                \n",
    "    def get_parameter(self, history, word):\n",
    "        \"\"\"\n",
    "        This function returns the categorical parameter associated with a certain word given a certain history.\n",
    "        :param history: a sequence of words (a tuple)\n",
    "        :param word: a word (a str)\n",
    "        :return: a float representing P(word|history)\n",
    "        \"\"\"\n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass\n",
    "\n",
    "    def count_ngrams(self, data_stream):\n",
    "        \"\"\"\n",
    "        This function should populate the attribute _count_table which should be understood as \n",
    "            - a python dict \n",
    "                - whose key is a history (a tuple of words)\n",
    "                - and whose value is itself a python dict (or defaultdict)\n",
    "                    - which maps a word (a string) to a count (an integer)\n",
    "        \n",
    "        This function will add counts to whatever counts are already stored in _count_table.\n",
    "        \n",
    "        :param data_stream: a generator as produced by `preprocess`\n",
    "        \"\"\"\n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass        \n",
    "                    \n",
    "    def solve_mle(self):\n",
    "        \"\"\"\n",
    "        This function should compute the attribute _prob_table which has the exact same structure as _count_table\n",
    "         but stores probability values instead of counts. \n",
    "        It can be seen as the collection of cpds of our model, that is, _prob_table\n",
    "            - maps a history (a tuple of words) to a dict where\n",
    "                - a key is a word (that extends the history forming an ngram)\n",
    "                - and the value is the probability P(word|history)                \n",
    "                \n",
    "        This function will replace whatever value _prob_table currently stores by the newly computed MLE solution.\n",
    "        \"\"\"\n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that from longer histories, smoothing with $\\alpha=1$ is unrealistic, that's why we generalise Laplace smoothing. As a rule of thumb we would want $\\alpha$ to decay exponentially fast with the length of the history, for example, if you use $a$ for unigrams, use $a/10$ or even $a/v$ for bigrams.\n",
    "\n",
    "**Tip** Test a few values and get a sense of what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"inter\"> Interpolation\n",
    "\n",
    "Laplace smoothing deals with unseen words for a seen history, but it is not the best strategy for unseen histories. A simple idea is to use language model interpolation in order to obtain more robust statistics for our ngrams. \n",
    "\n",
    "We interpolate language models $\\mathcal M_0, \\ldots, \\mathcal M_o$, where $\\mathcal M_j$ is a Markov model of order $j$, to obtain an interpolated $(o+1)$-gram language model. For the interpolation we use coefficients $\\lambda_0, \\ldots, \\lambda_o$ where\n",
    "\n",
    "* $0 < \\lambda_j < 1$\n",
    "* $\\sum_{j=0}^{o} \\lambda_j = 1$\n",
    "\n",
    "The probability of a sentence $x_1^n$ under the <a name=\"inter-snt-prob\">interpolated model</a> is\n",
    "\n",
    "\\begin{equation}\n",
    "(8) \\qquad P_S(x_1^n|n; \\mathcal M_0, \\ldots, \\mathcal M_o) = P_N(n) \\prod_{i=1}^n P_{X|H}(x_i|x_{<i}; \\mathcal M_0, \\ldots, \\mathcal M_o)\n",
    "\\end{equation}\n",
    "\n",
    "where the <a name=\"inter-factor\">interpolated factor is </a>\n",
    "\n",
    "\\begin{equation}\n",
    "(9) \\qquad P_{X|H}(x_i|x_{<i}; \\mathcal M_0, \\ldots, \\mathcal M_{n-1}) = \\sum_{j=0}^{o} \\lambda_j \\times P_{X|H}(x_i|x_{i-j}^{i-1}; \\mathcal M_j)\n",
    "\\end{equation}\n",
    "\n",
    "and $ P_{X|H}(x|h; \\mathcal M_j)$ is the probability of the $(j+1)$-gram suffix of $h \\circ \\langle x \\rangle$ under a model of order $j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, consider the sentence `here comes the sun`, for a $3$-gram LM (order $2$) we pad it `BOS BOS here comes the sun EOS` and compute interpolated factors:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\text{here} \\mid \\langle \\text{BOS, BOS} \\rangle) &= \\lambda_0 \\times P(\\text{here} \\mid \\langle \\rangle; \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P(\\text{here}\\mid \\langle \\text{BOS} \\rangle; \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P(\\text{here} \\mid \\langle \\text{BOS, BOS} \\rangle; \\mathcal M_2) \\\\\n",
    "P(\\text{comes}\\mid \\langle \\text{BOS, here} \\rangle) &= \\lambda_0 \\times P(\\text{comes}\\mid \\langle \\rangle; \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P(\\text{comes}\\mid\\langle \\text{here} \\rangle; \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P(\\text{comes}\\mid \\langle \\text{BOS, here} \\rangle; \\mathcal M_2) \\\\\n",
    "P(\\text{the}\\mid \\langle \\text{here, comes} \\rangle) &= \\lambda_0 \\times P(\\text{the}\\mid\\langle \\rangle; \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P(\\text{the}\\mid \\langle \\text{comes} \\rangle; \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P(\\text{the}\\mid\\langle \\text{here, comes} \\rangle; \\mathcal M_2) \\\\\n",
    "P(\\text{sun}\\mid \\langle \\text{comes, the} \\rangle) &= \\lambda_0 \\times P(\\text{sun}\\mid \\langle \\rangle; \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P(\\text{sun}\\mid \\langle \\text{the} \\rangle; \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P(\\text{sun}\\mid \\langle \\text{comes, the} \\rangle; \\mathcal M_2)  \\\\\n",
    "P(\\text{EOS}\\mid \\langle \\text{the, sun} \\rangle) &= \\lambda_0 \\times P(\\text{EOS}\\mid \\langle \\rangle; \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P(\\text{EOS}\\mid \\langle \\text{sun} \\rangle; \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P(\\text{EOS}\\mid \\langle \\text{the, sun} \\rangle; \\mathcal M_2) \n",
    "\\end{align}\n",
    "\n",
    "Then the probability of the sentence under the interpolation is proportional to\n",
    "\n",
    "\\begin{align}\n",
    "P_{S|N}(\\langle \\text{here, comes, the, sun, EOS}\\rangle|n) \n",
    "&= P(\\text{here} \\mid \\langle \\text{BOS, BOS} \\rangle) \\\\\n",
    "&\\times P(\\text{comes}\\mid \\langle \\text{BOS, here} \\rangle)  \\\\\n",
    "&\\times P(\\text{the}\\mid \\langle \\text{here, comes} \\rangle) \\\\\n",
    "&\\times P(\\text{sun}\\mid \\langle \\text{comes, the} \\rangle) \\\\\n",
    "&\\times P(\\text{EOS}\\mid \\langle \\text{the, sun} \\rangle)\n",
    "\\end{align}\n",
    "\n",
    "Let's try and implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3-8\" style=\"color:red\">**Exercise 3-8**</a> **[2 points]** Complete the class below which implements an interpolated language model.\n",
    "\n",
    "1. **[1 point]** start by completing the method `get_parameter` which computes the interpolated factor $P_{X|H}$ as shown in [Equation (9)](#inter-factor);\n",
    "2. **[1 point]** then complete the method `log_prob` which should use `get_parameter` to compute the log of the interpolated probability $P_{S|N}(x_1^n|n)$ as defined in [Equation (8)](#inter-snt-prob)\n",
    "\n",
    "If your implementation is correct you should be able to reproduce the following result\n",
    "\n",
    "```python\n",
    "lms = [\n",
    "    LaplaceLM(order=0, alpha=[1.]),  # unigram LM\n",
    "    LaplaceLM(order=1, alpha=[1., 0.01]),  # bigram LM\n",
    "    LaplaceLM(order=2, alpha=[1., 0.01, 0.001]),  # trigram LM\n",
    "    LaplaceLM(order=3, alpha=[1., 0.01, 0.001, 0.0001])   # 4-gram LM\n",
    "]\n",
    "# train our models\n",
    "for lm in lms:\n",
    "    lm.count_ngrams(preprocess('sec02-21.raw'))\n",
    "    lm.solve_mle()\n",
    "\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), InterpolatedLM(lms[0:1], [1.])))\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), InterpolatedLM(lms[0:2], [0.5, 0.5])))\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), InterpolatedLM(lms[0:3], [0.5, 0.3, 0.2])))\n",
    "print(log_perplexity(make_test_generator('sec00.raw'), InterpolatedLM(lms, [0.5, 0.3, 0.15, 0.05])))\n",
    "```\n",
    "\n",
    "which should produce\n",
    "\n",
    "```python\n",
    "7.06497838227\n",
    "6.24567919494\n",
    "6.72833206642\n",
    "7.07763692454\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InterpolatedLM(LM):\n",
    "    \n",
    "    def __init__(self, lms, weights):\n",
    "        \"\"\"\n",
    "        This class should interpolate language models, \n",
    "            there are certain conditions that they must hold.\n",
    "            \n",
    "        :params lms: a list of language models where the lms[i] should have order i\n",
    "        :params weights: a list of positive weights that should sum to 1.0        \n",
    "        \"\"\"\n",
    "        if not lms:\n",
    "            raise ValueError('I need at least 1 language model')\n",
    "        if not all(0 < w < 1 for w in weights) and sum(weights) != 1.0:\n",
    "            raise ValueError('LM weights must sum to 1')\n",
    "        # Let's check that we have the LMs we need\n",
    "        for i, lm in enumerate(lms):\n",
    "            if lm.order() != i:\n",
    "                raise ValueError('Interpolation requires the ith LM to be of order i-1')\n",
    "        self._max_order = lms[-1].order()  # the maximum order\n",
    "        self._lms = lms\n",
    "        self._weights = weights\n",
    "        \n",
    "    def order(self):\n",
    "        return self._max_order\n",
    "    \n",
    "    def print_count_table(self, output_stream=sys.stdout):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "                \n",
    "    def print_prob_table(self, output_stream=sys.stdout):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "                \n",
    "    def preprocess_history(self, history):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "            \n",
    "    def cpd_items(self, history):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "        \n",
    "    def count_ngrams(self, data_stream):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "                    \n",
    "    def solve_mle(self):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "                \n",
    "    def get_parameter(self, history, word):\n",
    "        \"\"\"\n",
    "        This function should return the interpolated factor P(X=w|H=h) as defined in Equation (9) above.\n",
    "    \n",
    "        :param history: a sequence of words (a tuple)\n",
    "        :param word: a word (a str)\n",
    "        :return: a float representing P(word|history) in the interpolated model\n",
    "        \"\"\"       \n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass\n",
    "    \n",
    "    def log_prob(self, sentence):\n",
    "        \"\"\"\n",
    "        Compute the log probability of a sentence under this model. \n",
    "                \n",
    "        input: \n",
    "            sentence: a sequence of tokens\n",
    "        output:\n",
    "            log probability\n",
    "        \"\"\"\n",
    "        # ***TYPE YOUR SOLUTION***\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we start with a unigram LM alone, we then try interpolating a unigram LM and a bigram LM and improve perplexity considerably. While our unigram model chooses the next word from on average `np.exp(7.06)` words, the bigram language model chooses from on average `np.exp(6.24)` words.\n",
    "\n",
    "Curiously, further interpolating trigram and fourgram LM does not really help much. This has again to do with data sparsity: our training corpus is not very large and therefore most 3-grams and 4-grams are quite rare. If there's little overlap between training and test in terms of 4-grams, the 4-gram terms in the interpolation will be mostly dominated by Laplace smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"ex3-9\" style=\"color:red\">**Exercise 3-9**</a> **[3 points]** \n",
    "\n",
    "* **[1 point]** Train two interpolated models: one on PTB data `sec02-21.raw`, another on Beatles lyrics `beatles.txt` (this file does not include the song Eleanor Rigby neither Ask Me Why).\n",
    "    * Interpolate models of order 0, 1, and 2\n",
    "    * Use Laplace models with alpha 1.0\n",
    "    * Use interpolation weights [0.1, 0.3, 0.6]\n",
    "\n",
    "\n",
    "* **[1 point]** Compare the log-perplexity each model assigns to \n",
    "    * `sec00.raw` \n",
    "    * `eleanor-rigby.txt`\n",
    "    * `ask-me-why.txt`\n",
    "\n",
    "* **[1 point]** Explain the results you obtain. In particular, try to explain why the PTB models performs quite differently if tested with `Ask Me Why` or `Eleanor Rigby`.\n",
    "\n",
    "\n",
    "**Grading guidelienes**\n",
    "\n",
    "In the last part students should discuss aspects such as `sec00.raw` is very similar in nature to `sec02-21.raw`, thus the PTB model can explain it better (lower perplexity) than the Beatles model. Eleanor Rigby is quite well explained by the PTB model (even better than the Beatles model), perhaps because the vocabulary is not all too peculiar, and the text reads a bit like a report about Eleanor and Father McKenzie. Ask Me Why has a vocabulary which is more unusual of newswire (which is the domain of the PTB corpus). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
