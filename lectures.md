---
layout: default
title: Lectures
menu: yes
---

# Slides

* [Week 1-a](lectures/week1-a.pdf): intro to the course, what's NLP about, overview of problems
* [Week 1-b](lectures/week1-b.pdf): probability theory, maximum likelihood estimation 
    * Further reading: [Notes on MLE for Bernoulli and Categorical](//github.com/wilkeraziz/notes/blob/master/machine-learning/MLE/main.pdf)
* Week 2a and 2b:
    * Before class: [Notes on directed graphical models](//github.com/wilkeraziz/notes/blob/master/machine-learning/PGM/main.pdf) and [slides on directed graphical models](lectures/pgms.pdf)
    * [Week 2-ab](lectures/week2.pdf): probability of a sentence, n-gram language models, smoothing, perplexity 
    * Textbook: Ch4 (2nd edition) or Ch3 (3rd edition)
* Week 3-a and 3-b:
    * [HMMs](lectures/week3-a.pdf): parts of speech, hidden Markov models (HMM) for language modelling and POS tagging, Viterbi algorithm
        * Marginal probabilities explained: [errata](lectures/forward-errata.pdf) and [notebook](labs/forward/Forward.ipynb)
    * Textbook: Ch5 (2nd edition) or Ch8 (3rd edition)
* Week 4: no lectures
* Week 5
    * [Probabilistic context free grammars](lectures/week5-a.pdf)
    * Parsing and inside algorithm: [pdf](lectures/week5-b.pdf), [keynote](lectures/week5-b.key)
    * Textbook: Ch 12-14 (2n edition)
* Week 6
    * Feature-rich models: [logistic CPDs](lectures/week6-a.pdf)
    * Applications of logistic regression to NLP
* Week 7:
    * Lexical semantics
* Week 8: final exam

